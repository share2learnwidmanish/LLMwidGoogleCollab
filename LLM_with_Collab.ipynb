{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running Open Source LLMs via Ollama API on Google Colab\n",
        "--- \n",
        "This notebook demonstrates how to set up an **Ollama** server within a Google Colab environment to perform API calls to Small Language Models (SLMs) for free. \n",
        "\n",
        "### Hardware Specs:\n",
        "* **GPU:** Tesla T4 (~15GB VRAM)\n",
        "* **Model Recommendation:** Use models under 4B parameters (e.g., Llama 3.2 1B, Phi-4-mini) to avoid OOM errors."
      ],
      "metadata": { "id": "header" }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Step 1: Prep the Environment\n",
        "Colab‚Äôs base image requires system-level utilities like `zstd` to unpack Ollama‚Äôs binaries. We also install `pciutils` to ensure the GPU is detectable."
      ],
      "metadata": { "id": "step1_md" }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!apt-get update -qq && apt-get install -y -qq zstd pciutils\n",
        "\n",
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": { "id": "step1_code" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 2: Launch the Background Daemon\n",
        "Ollama must run as a background service so that the notebook remains interactive for your API calls."
      ],
      "metadata": { "id": "step2_md" }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Start Ollama server in the background\n",
        "with open('ollama.log', 'w') as f:\n",
        "    subprocess.Popen(['ollama', 'serve'], stdout=f, stderr=f)\n",
        "\n",
        "time.sleep(5) # Allow initialization\n",
        "print(\"Ollama server is active.\")"
      ],
      "metadata": { "id": "step2_code" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Step 3: Pull Your Model\n",
        "We are using `llama3.2:1b` for its efficiency on the T4 GPU."
      ],
      "metadata": { "id": "step3_md" }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2:1b"
      ],
      "metadata": { "id": "step3_code" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó Step 4: Perform the API Call\n",
        "Using the `requests` library, we hit the local endpoint. Note that we access the payload via `.json()['response']` to extract the LLM's answer."
      ],
      "metadata": { "id": "step4_md" }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://localhost:11434/api/generate\"\n",
        "payload = {\n",
        "    \"model\": \"llama3.2:1b\",\n",
        "    \"prompt\": \"Explain the concept of quantum entanglement in one sentence.\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"LLM Response:\")\n",
        "    print(result['response'])\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")"
      ],
      "metadata": { "id": "step4_code" },
      "execution_count": null,
      "outputs": []
    }
  ]
}